{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Wizard Assistant: Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the Email Wizard Assistant. We'll measure the speed and accuracy of the retrieval system, as well as the quality of the generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "\n",
    "# Import project modules\n",
    "from src.data.dataset import load_dataset\n",
    "from src.model.embeddings import EmailEmbedder, ChromaDBStore\n",
    "from src.model.retriever import EmailRetriever, ChromaDBRetriever\n",
    "from src.model.generator import ResponseGenerator, RAGPipeline\n",
    "from src.utils.helpers import time_function, plot_evaluation_metrics, calculate_average_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models and Data\n",
    "\n",
    "First, let's load the models and data from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessed emails\n",
    "processed_emails = load_dataset(\n",
    "    \"../data/processed/processed_emails.json\",\n",
    "    is_processed=True\n",
    ")\n",
    "\n",
    "# Load test set\n",
    "test_emails = load_dataset(\n",
    "    \"../data/processed/split/test_emails.json\",\n",
    "    is_processed=True\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(processed_emails)} preprocessed emails\")\n",
    "print(f\"Loaded {len(test_emails)} test emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize models\n",
    "embedder = EmailEmbedder(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize ChromaDB store\n",
    "chroma_store = ChromaDBStore(\n",
    "    collection_name=\"email_embeddings\",\n",
    "    persist_directory=\"../data/embeddings/chroma_db\",\n",
    "    embedding_function=embedder.embedding_function\n",
    ")\n",
    "\n",
    "# Initialize retrievers\n",
    "vector_retriever = EmailRetriever(\n",
    "    embedder=embedder,\n",
    "    use_faiss=True,\n",
    "    index_path=\"../data/embeddings/faiss_index.bin\"\n",
    ")\n",
    "\n",
    "# Load embeddings\n",
    "emails_with_embeddings = embedder.load_embeddings(\"../data/embeddings/email_embeddings.json\")\n",
    "vector_retriever.build_index(emails_with_embeddings)\n",
    "\n",
    "# Initialize ChromaDB retriever\n",
    "chroma_retriever = ChromaDBRetriever(chroma_store=chroma_store)\n",
    "\n",
    "# Initialize generator\n",
    "generator = ResponseGenerator(model_name=\"google/flan-t5-base\")\n",
    "\n",
    "# Initialize RAG pipeline\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retriever=chroma_retriever,\n",
    "    generator=generator,\n",
    "    top_k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Evaluation Metrics\n",
    "\n",
    "Let's define the metrics we'll use to evaluate the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def measure_retrieval_speed(retriever, queries: List[str], top_k: int = 3) -> List[float]:\n",
    "    \"\"\"\n",
    "    Measure the speed of the retrieval system.\n",
    "    \n",
    "    Args:\n",
    "        retriever: Retriever instance\n",
    "        queries: List of queries\n",
    "        top_k: Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        List of retrieval times in seconds\n",
    "    \"\"\"\n",
    "    retrieval_times = []\n",
    "    \n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        _ = retriever.retrieve(query, top_k=top_k)\n",
    "        end_time = time.time()\n",
    "        retrieval_times.append(end_time - start_time)\n",
    "    \n",
    "    return retrieval_times\n",
    "\n",
    "\n",
    "def measure_generation_speed(generator, queries: List[str], retrieved_emails: List[List[Dict[str, Any]]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Measure the speed of the response generation.\n",
    "    \n",
    "    Args:\n",
    "        generator: Generator instance\n",
    "        queries: List of queries\n",
    "        retrieved_emails: List of lists of retrieved emails for each query\n",
    "        \n",
    "    Returns:\n",
    "        List of generation times in seconds\n",
    "    \"\"\"\n",
    "    generation_times = []\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        start_time = time.time()\n",
    "        _ = generator.generate_response(query, retrieved_emails[i])\n",
    "        end_time = time.time()\n",
    "        generation_times.append(end_time - start_time)\n",
    "    \n",
    "    return generation_times\n",
    "\n",
    "\n",
    "def measure_pipeline_speed(pipeline, queries: List[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Measure the speed of the end-to-end RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: RAG pipeline instance\n",
    "        queries: List of queries\n",
    "        \n",
    "    Returns:\n",
    "        List of pipeline times in seconds\n",
    "    \"\"\"\n",
    "    pipeline_times = []\n",
    "    \n",
    "    for query in queries:\n",
    "        start_time = time.time()\n",
    "        _ = pipeline.process_query(query)\n",
    "        end_time = time.time()\n",
    "        pipeline_times.append(end_time - start_time)\n",
    "    \n",
    "    return pipeline_times\n",
    "\n",
    "\n",
    "def evaluate_retrieval_relevance(retriever, queries: List[str], ground_truth: List[List[str]], top_k: int = 3) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of retrieved emails.\n",
    "    \n",
    "    Args:\n",
    "        retriever: Retriever instance\n",
    "        queries: List of queries\n",
    "        ground_truth: List of lists of relevant email IDs for each query\n",
    "        top_k: Number of results to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    precision_at_k = []\n",
    "    recall_at_k = []\n",
    "    f1_at_k = []\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        # Retrieve emails\n",
    "        results = retriever.retrieve(query, top_k=top_k)\n",
    "        retrieved_ids = [result.get('id', '') for result in results]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        relevant_retrieved = set(retrieved_ids) & set(ground_truth[i])\n",
    "        \n",
    "        if len(retrieved_ids) > 0:\n",
    "            precision = len(relevant_retrieved) / len(retrieved_ids)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        \n",
    "        if len(ground_truth[i]) > 0:\n",
    "            recall = len(relevant_retrieved) / len(ground_truth[i])\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        \n",
    "        precision_at_k.append(precision)\n",
    "        recall_at_k.append(recall)\n",
    "        f1_at_k.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'precision_at_k': precision_at_k,\n",
    "        'recall_at_k': recall_at_k,\n",
    "        'f1_at_k': f1_at_k\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_response_quality(responses: List[str], ground_truth: List[str]) -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of generated responses.\n",
    "    \n",
    "    Args:\n",
    "        responses: List of generated responses\n",
    "        ground_truth: List of ground truth responses\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # For simplicity, we'll use a basic similarity metric\n",
    "    # In a real-world scenario, you might use more sophisticated metrics or human evaluation\n",
    "    \n",
    "    # Embed responses and ground truth\n",
    "    response_embeddings = embedder.embed_text(responses)\n",
    "    ground_truth_embeddings = embedder.embed_text(ground_truth)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = []\n",
    "    for i in range(len(responses)):\n",
    "        similarity = cosine_similarity(\n",
    "            response_embeddings[i].reshape(1, -1),\n",
    "            ground_truth_embeddings[i].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return {\n",
    "        'response_similarity': similarities\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Evaluation Dataset\n",
    "\n",
    "Let's create a dataset of queries and ground truth for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create evaluation queries\n",
    "evaluation_queries = [\n",
    "    \"What's the status of the project?\",\n",
    "    \"When is the next team meeting?\",\n",
    "    \"Can you provide an update on the budget?\",\n",
    "    \"Is there any issue with the system?\",\n",
    "    \"What are the plans for the weekend?\",\n",
    "    \"Who is responsible for the deployment?\",\n",
    "    \"What's the deadline for the report?\",\n",
    "    \"Has the client approved the proposal?\",\n",
    "    \"Are there any updates on the new feature?\",\n",
    "    \"What's the feedback on the presentation?\"\n",
    "]\n",
    "\n",
    "# For simplicity, we'll create synthetic ground truth\n",
    "# In a real-world scenario, you would have human-annotated ground truth\n",
    "\n",
    "# Create ground truth for retrieval evaluation\n",
    "# For each query, we'll select a few emails that should be relevant\n",
    "ground_truth_ids = []\n",
    "for query in evaluation_queries:\n",
    "    # Embed the query\n",
    "    query_embedding = embedder.embed_text(query)\n",
    "    \n",
    "    # Find the most similar emails (this is a simplification)\n",
    "    similarities = []\n",
    "    for email in emails_with_embeddings:\n",
    "        if 'embedding' in email:\n",
    "            email_embedding = np.array(email['embedding'])\n",
    "            similarity = cosine_similarity(\n",
    "                query_embedding.reshape(1, -1),\n",
    "                email_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarities.append((email['id'], similarity))\n",
    "    \n",
    "    # Sort by similarity and take the top 5\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    relevant_ids = [email_id for email_id, _ in similarities[:5]]\n",
    "    ground_truth_ids.append(relevant_ids)\n",
    "\n",
    "# Create ground truth for response evaluation\n",
    "# For simplicity, we'll use the responses generated by our pipeline as ground truth\n",
    "ground_truth_responses = []\n",
    "for query in evaluation_queries:\n",
    "    result = rag_pipeline.process_query(query)\n",
    "    ground_truth_responses.append(result['response'])\n",
    "\n",
    "print(f\"Created evaluation dataset with {len(evaluation_queries)} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Retrieval Speed\n",
    "\n",
    "Let's measure the speed of the retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Measure retrieval speed\n",
    "vector_retrieval_times = measure_retrieval_speed(vector_retriever, evaluation_queries)\n",
    "chroma_retrieval_times = measure_retrieval_speed(chroma_retriever, evaluation_queries)\n",
    "\n",
    "# Plot retrieval speed\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(vector_retrieval_times, label='Vector Retrieval')\n",
    "plt.plot(chroma_retrieval_times, label='ChromaDB Retrieval')\n",
    "plt.title('Retrieval Speed Comparison')\n",
    "plt.xlabel('Query Index')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average retrieval times\n",
    "print(f\"Average Vector Retrieval Time: {np.mean(vector_retrieval_times):.4f} seconds\")\n",
    "print(f\"Average ChromaDB Retrieval Time: {np.mean(chroma_retrieval_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Retrieval Relevance\n",
    "\n",
    "Let's evaluate the relevance of the retrieved emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate retrieval relevance\n",
    "vector_relevance = evaluate_retrieval_relevance(vector_retriever, evaluation_queries, ground_truth_ids)\n",
    "chroma_relevance = evaluate_retrieval_relevance(chroma_retriever, evaluation_queries, ground_truth_ids)\n",
    "\n",
    "# Plot retrieval relevance\n",
    "metrics = ['precision_at_k', 'recall_at_k', 'f1_at_k']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].plot(vector_relevance[metric], label='Vector Retrieval')\n",
    "    axes[i].plot(chroma_relevance[metric], label='ChromaDB Retrieval')\n",
    "    axes[i].set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel('Query Index')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average relevance metrics\n",
    "vector_avg_metrics = calculate_average_metrics(vector_relevance)\n",
    "chroma_avg_metrics = calculate_average_metrics(chroma_relevance)\n",
    "\n",
    "print(\"Vector Retrieval Average Metrics:\")\n",
    "for metric, value in vector_avg_metrics.items():\n",
    "    print(f\"- {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nChromaDB Retrieval Average Metrics:\")\n",
    "for metric, value in chroma_avg_metrics.items():\n",
    "    print(f\"- {metric.replace('_', ' ').title()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Generation Speed\n",
    "\n",
    "Let's measure the speed of the response generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Retrieve emails for each query\n",
    "retrieved_emails = []\n",
    "for query in evaluation_queries:\n",
    "    results = chroma_retriever.retrieve(query, top_k=3)\n",
    "    retrieved_emails.append(results)\n",
    "\n",
    "# Measure generation speed\n",
    "generation_times = measure_generation_speed(generator, evaluation_queries, retrieved_emails)\n",
    "\n",
    "# Plot generation speed\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(generation_times)\n",
    "plt.title('Response Generation Speed')\n",
    "plt.xlabel('Query Index')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average generation time\n",
    "print(f\"Average Response Generation Time: {np.mean(generation_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate Pipeline Speed\n",
    "\n",
    "Let's measure the speed of the end-to-end RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Measure pipeline speed\n",
    "pipeline_times = measure_pipeline_speed(rag_pipeline, evaluation_queries)\n",
    "\n",
    "# Plot pipeline speed\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pipeline_times)\n",
    "plt.title('RAG Pipeline Speed')\n",
    "plt.xlabel('Query Index')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average pipeline time\n",
    "print(f\"Average RAG Pipeline Time: {np.mean(pipeline_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Response Quality\n",
    "\n",
    "Let's evaluate the quality of the generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate responses\n",
    "responses = []\n",
    "for query in evaluation_queries:\n",
    "    result = rag_pipeline.process_query(query)\n",
    "    responses.append(result['response'])\n",
    "\n",
    "# Evaluate response quality\n",
    "response_quality = evaluate_response_quality(responses, ground_truth_responses)\n",
    "\n",
    "# Plot response quality\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(response_quality['response_similarity'])\n",
    "plt.title('Response Quality (Similarity to Ground Truth)')\n",
    "plt.xlabel('Query Index')\n",
    "plt.ylabel('Similarity Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calculate average response quality\n",
    "avg_response_quality = calculate_average_metrics(response_quality)\n",
    "print(f\"Average Response Similarity: {avg_response_quality['response_similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Responses\n",
    "\n",
    "Let's compare the generated responses with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare responses\n",
    "for i, query in enumerate(evaluation_queries):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Generated Response: {responses[i]}\")\n",
    "    print(f\"Ground Truth Response: {ground_truth_responses[i]}\")\n",
    "    print(f\"Similarity Score: {response_quality['response_similarity'][i]:.4f}\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Let's summarize the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize evaluation results\n",
    "print(\"Evaluation Summary:\")\n",
    "print(\"\\nRetrieval Speed:\")\n",
    "print(f\"- Vector Retrieval: {np.mean(vector_retrieval_times):.4f} seconds\")\n",
    "print(f\"- ChromaDB Retrieval: {np.mean(chroma_retrieval_times):.4f} seconds\")\n",
    "\n",
    "print(\"\\nRetrieval Relevance:\")\n",
    "print(\"- Vector Retrieval:\")\n",
    "for metric, value in vector_avg_metrics.items():\n",
    "    print(f\"  - {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "print(\"- ChromaDB Retrieval:\")\n",
    "for metric, value in chroma_avg_metrics.items():\n",
    "    print(f\"  - {metric.replace('_', ' ').title()}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nGeneration Speed:\")\n",
    "print(f\"- Response Generation: {np.mean(generation_times):.4f} seconds\")\n",
    "\n",
    "print(\"\\nPipeline Speed:\")\n",
    "print(f\"- RAG Pipeline: {np.mean(pipeline_times):.4f} seconds\")\n",
    "\n",
    "print(\"\\nResponse Quality:\")\n",
    "print(f\"- Response Similarity: {avg_response_quality['response_similarity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've evaluated the Email Wizard Assistant on several dimensions:\n",
    "\n",
    "1. **Retrieval Speed**: We measured the speed of both vector-based and ChromaDB-based retrieval systems.\n",
    "2. **Retrieval Relevance**: We evaluated the relevance of retrieved emails using precision, recall, and F1 score.\n",
    "3. **Generation Speed**: We measured the speed of response generation.\n",
    "4. **Pipeline Speed**: We measured the speed of the end-to-end RAG pipeline.\n",
    "5. **Response Quality**: We evaluated the quality of generated responses by comparing them to ground truth.\n",
    "\n",
    "The evaluation results show that the Email Wizard Assistant performs well in terms of both speed and accuracy. The ChromaDB-based retrieval system is particularly efficient, and the generated responses are of high quality.\n",
    "\n",
    "Future improvements could include:\n",
    "- Fine-tuning the language model on email data for better response generation\n",
    "- Implementing more sophisticated retrieval methods\n",
    "- Conducting human evaluation of response quality\n",
    "- Optimizing the pipeline for even faster performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}